\documentclass[a4paper,11pt]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[unicode=true]{hyperref}
\hypersetup{
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls


\setlength{\emergencystretch}{3em}  % prevent overfull lines

\setlength{\parskip}{0.6\baselineskip}%
\setlength\parindent{0pt}

% set margins
\RequirePackage{geometry}
 \geometry{
 a4paper,
 left=25mm,
 top=25mm,
 right=25mm,
 bottom=25mm
 }

\begin{document}

\title{FUME emission processor user guide}
\maketitle

\tableofcontents

\newpage


\section{Introduction}\label{introduction}

The emission processor FUME (Flexible Universal Processor for Modeling Emissions) has been
developed by a consorcium of Czech Technical University (CIIRC and
Transportation Faculty), Czech Hydrometeorological Institute and Department of
Atmospheric Physics, Faculty of Mathematics and Physics, Charles
University, Prague, under support of Technological Agency of the Czech
Republic.

The main and primary task of the FUME emission processor is to produce inputs for air quality
models. The software has been designed to process data from different sources they are in different spatial resolution and geometries, in different file formats and units with different naming conventions, etc. with as low user data preparation as possible. The inputs are preprocessed and unified to do chemical speciation, space and time disaggregation of emissions. Then the data are exported as emission fluxes time series in model grid as CTM-ready inputs. 
FUME supports GIS technology and is not restricted to regular 2D or 3D grids and its
outputs thus may be used for survey and reporting tasks, e.g. for administrative units or other
partitioning defined by the user.

The functionalities include:

\begin{quote}
\begin{itemize}
\item
  nontrivial ways of combining different data sources
\item
  source (spatial) masking and (attribute) filtering capabilities
\item
  link to GIS data
\item
  support for air quality models CMAQ, CAMx and PALM
\item
  possibility to include external modules (support for MEGAN biogenic emission model is implemented)
\item
  coupling with numeric weather models ALADIN and WRF
\item
  emission scenarios
\item
  simple visualisation possibility
\end{itemize}
\end{quote}

The FUME processor is written in Python and PostgreSQL/PostGIS. Python
scripts operate on the database and GIS data and the system is highly
flexible and configurable.


\section{Structure of the emission processing
system}\label{structure}

The emission processor is a complex, heterogeneous system consisting of
many submodules of different character. These submodules treat different tasks needed during the emission processing for air quality models.

The processor has been designed so as to
keep the vast majority of the code independent of any particular air
quality model. The coupling to the target AQ model is left to the very
end of processing chain. Thus the outputs of FUME may be used for
Eulerian CTMs (CAMx and CMAQ supported so far) but the adaptation to
Lagrangian and Gaussian models may be achieved with no overhead efforts.
This approach has been applied for processing sources as well as for
spatial and temporal processing of emissions, speciations and final
generation of emission flows.

The processor implements the widely accepted disaggregation model for
emission flows:

T(p,l,t,s) = Sum\_\{i,j\} {[} Z(i,j).q\_p (i,j).q\_l (i,j).q\_t
(i,j).q\_s (i,j) {]},

where T(p,l,t,s) is output emission flow for a given polygon p, vertical
level l, time t and output species s,

Z(i,j) is a primary emission of emitted species j from source i
(typically in tons per year) q p (i,j) disaggregation coefficient for
emitted species j from source i into polygonu p q l (i,j) disaggregation
coefficient for emitted species j from source i into vertical level l q
t (i,j) disaggregation coefficient for emitted species j from source i
into time t q s (i,j) disaggregation coefficient for emitted species j
from source i into output species s.

For certain types of emission sources this model is not appropriate,
e.g. biogenic emissions, emissions from lightning etc. For those cases
special models exist, which are based on domain knowledge. These are
naturally out of scope of FUME and an interface has been built into FUME
for external models. Currently the MEGAN biogenic emission model has a
built-in support in FUME.

Emisson processor uses server-client architecture to perform most of the
tasks. Server holds the database with input and auxiliary data, client
is used to configure the tasks which should lead to transforming the
input data to desired emission outputs (typically for the use by CTM).

\emph{PLUME RISE?}

\emph{NH3, lightning?}

To accomplish a simulation, the user needs to

\begin{itemize}
\item
  define the output grid
\item
  manage the sources specification (see ``Treatment of emission sources
  below''). All of the specified sources will be included into
  processing
\item
  define the transformation chains. During this step any filtering and
  masking of the sources will be done
\item
  run the simulation. All emission outputs will be summed up and added
  to the user grid. Two files will be output (unless supressed) - Area
  emissions and Point emissions.
\end{itemize}

It is the responsibility of the user to make sure that no erratic
overlapping of emission outputs will occur. For example, if one uses
finer inventories for the domain of interest while for the background a
coarser inventory is available, the user has to mask out the finer
domain from the coarser inventory. So far, the system doesn't detect
these situations nor does produce any warnings of this kind.

Typical workflow contains following steps:

\begin{itemize}
\item
  only done once: - downloading code - setting up the database server
\item
  done regularly: - configuring processor - running the processor
\end{itemize}

\section{Installation}\label{installation}
Os windows

For the context of the user documentation following conventions are
being used:

FUME\_ROOT specifies the FUME codebase directory (typically the local
git repository clone) FUME\_CASE specifies the run directory for the
user case outside of the FUME codebase

Prerequisities:

\begin{itemize}
\item
  python3 (3.5 or higher)
\item
  database server: packages postgresql, postgresql-contrib 9.5 or newer
\item
  postgis (version 1.5.3 tested)
\end{itemize}

Other libraries for python

\begin{itemize}
\item
  pip, pyproj, psycopg2, pygrib, netcdf4, gdal, pint, configobj, scipy,
  pytz, numpy, python-cdo
\end{itemize}

Optional libraries for plotting (only necessary when EmissPlotter
postprocessors are used)

\begin{itemize}
\item
  \textbf{matplotlib}, basemap, shapely, fiona
\end{itemize}

Some of the python packages mentioned above may not be present depending
on distribution. They may be installed via pip or the native package
manager, e.g. on SUSE:

\begin{verbatim}
 pip install configobj zypper install python3-psycopg2
\end{verbatim}

Extension for pip3: 

\begin{verbatim}
 pip3 install pytz
\end{verbatim}

If using the anaconda distribution, it may be necessary to use
conda-forge:

\begin{verbatim}
 conda install -c conda-forge pygrib pint gdal
\end{verbatim}

On some linux distributions a repository must be added, e.g. on SUSE
leap 42.2:

\begin{verbatim}
zypper addrepo
\url{http://download.opensuse.org/repositories/Application:Geo:Staging/openSUSE_Leap_42.2/Application:Geo:Staging.repo}
zypper refresh zypper install postgresql94-postgis zypper refresh zypper
install postgresql94-postgis
\end{verbatim}

Tools for administration and visualisation

\begin{itemize}
\item
  qgis (+ python-qgis) for data manipulation
\item
  pgadmin3 (access to databases)
\end{itemize}

Installation of FUME from github (nebo z jineho mista?)

\begin{itemize}
\item
  downloading FUME git pull from git repository of emisproc
\item
  budeme umoznovat tarball ?
\end{itemize}

\section{Quick start}\label{quick-start}

If not already started, start (possibly as root) the postgres sql server

\begin{verbatim}
 systemctl start postgresql 
\end{verbatim}
or possibly with version number: 
\begin{verbatim}
 systemctl start postgresql-9.6
\end{verbatim}


Create database:

\begin{itemize}
\item
  You have to either copy intialization scripts in directory
  \emph{server} somewhere where user postgres can read it or give user
  postgress read access to server directory, e.g.
\end{itemize}

\begin{verbatim}
cp -ra emisproc/server /var/lib/pgsql/
\end{verbatim}

as root on SUSE-based linuxes

\begin{itemize}
\item
  Switch to user ``postgres'':
\end{itemize}

\begin{verbatim}
sudo -u postgres -i
\end{verbatim}

or if you don't use sudo:

\begin{verbatim}
su; su - postgres
\end{verbatim}

\begin{itemize}
\item
  run the script
\end{itemize}

\begin{verbatim}
$FUME_ROOT/server/ep_create_database.sh
\end{verbatim}

Alternatively instead of running the script:

\begin{verbatim}
sudo su - postgres createuser -P username \# asks for password createdb
-E UTF8 -O username [database-name] createlang postgis
[database-name] createlang postgis_topology [database-name]
createlang intarray [database-name] psql -d [user] -c ``grant
all on database [database-name] to [user] with grant option;''
(PostgreSQL syntax for adding full privileges to [user] for database
[database-name]) psql -d [database-name] -c ``grant all on
spatial_ref_sys to [user];''
\end{verbatim}

logout from the database, the rest is done under the account \emph{username}

\begin{verbatim}
cd $FUME_ROOT/server

psql -h hostname -p port -U username [-W] -d dbname -f ep_create_database.sql
\end{verbatim}

Run the emission processor:

Create a working directory FUME\_CASE (can be changed). For the test
case, copy example configuration files from
\$FUME\_ROOT/doc/example-config

\begin{verbatim}
cd $FUME_CASE
\end{verbatim}

and run, generally

\begin{verbatim}
python3 $FUME_ROOT/client/fume [-c main_config] [-w workflow_config]
\end{verbatim}

In particular for the test case provided with the distribution 

\begin{verbatim}
python3 $FUME_ROOT/client/fume -c fume_run.conf -w fume_workflow.conf
\end{verbatim}

\section{Database}\label{database}
It is completely possible to use FUME without any direct interaction with the database. For experienced/interested user there are some tips.

The database structure of an emission database contains several schemas:

\begin{itemize}
\item
  case schema
\item
  configuration schema
\item
  sources schema
\item
  static schema
\item
  topology schema
\end{itemize}

Normally the user doesn't need to change the schemas. Currently the
schemas conf\_test, static\_test, sources\_test and case\_test are
provided and all what is needed is to name them in the main
configuration file.

Advanced users may create their own schemas and specify single schemas
for each run in the main configuration file (fume\_run.conf), e.g.

In the sources schema, each ``atomic'' source in the raw file has a
unique identifier of source (source\_orig\_id) and geometry
(geom\_orig\_id). Different esets are then created by grouping the
identifiers. These tables are created during processing:
{[}schema{]}.ep\_in\_sources, {[}schema{]}.ep\_in\_emissions,
{[}schema{]}.ep\_in\_geometries.

Under ordinary circumstances it is not necessary to optimize any
settings or configuration of postgres database. Had the import of data
or saving data to tables taken too long, it is possible to switch off
the autovacuum daemon in /var/lib/postgresql.conf.

\begin{verbatim}
 conf_schema = conf_test 
 static_schema = static_test 
 source_schema = sources_test 
 case_schema = case_test 
\end{verbatim}

This means (among others) that the user can modify some parts of the
simulation without the necessity of running all processing from scratch.

\section{Input data}\label{input-data}

Input data are typically located in the directory \$FUME\_CASE, which is
the user case run directory. The input directory contains inventories,
meteo inputs and static data like speciacions and time schedules.

\subsection{Example test case}\label{example-test-case}

For the test case provided with the distribution, input data are
downloadable from \ldots{} After the download you should see the
following structure:

\begin{verbatim}
example_data
../input
../output
fume_run.conf
fume_transformations.conf
fume_workflow.conf
\end{verbatim}


\section{User configuration}\label{user-configuration}

As mentioned above, user configuration typically resides in the
\$FUME\_CASE directory, however, the user can specify any path to the
main configuration file with the \texttt{-c} option (defaults to
\texttt{fume\_run.conf}) and the path to the workflow configuration file
with the \texttt{-w} option (defaults to \texttt{fume\_workflow.conf}).
In the distribution, example configuration files are provided in the
doc/example-config directory and full configuration specification files
(templates) are located in the directory client/conf.

There is one main configuraion file, typically named fume\_run.conf. The
name may be supplied via the \texttt{-c} option to the main executable
\texttt{fume}.

A template for this file including all valid specifications is the file
\texttt{client/conf/configspec.conf}.

There are three other configuration files, namely:

\emph{VYJASNIT STRUKTURU konfiguraku} \emph{mohou se uz includovat?}

\begin{itemize}
\item
  fume\_workflow.conf
\item
  fume\_transformations.conf
\item
  configspec-sources.conf
\item
  configspec-cmaq.conf
\end{itemize}

The definitions (syntax) of any valid transformation is in the file
config/transformations/configspec-transformations.conf. Using these
definitions, the users may create their own file e.g.
fume\_transformations.conf where user-defined transformations are
defined. These may then appear in transformation chains (see below).

configspec.conf contains the following specifications:

\begin{itemize}
\item
  paths to input data, in particular inventories, static data,
  meteorology files, biogenic emisssion files
\item
  parameters for connecting the PostgreSQL database
\item
  parameters for connecting the PostgreSQL database and names of schemas
\item
  specifications of grids in meteorological inputs as well as output
  user grid
\item
  projection parameters (tady neni jasne ktereho gridu se projekce tyka)
\item
  output specifications
\item
  transformation chains to be performed
\end{itemize}

\textbf{Treatment of emission sources}

In order to minimize the efforts needed for incorporation of
user-contributed or in-house inventories of the users, the system
enables different formats of inventories, different projections of each
source etc.

The main configuration file responsible for processing of inventories
(raw sources) is the file \emph{inventory\_input.txt} typically located
in the FUME\_CASE directory (full path including file name may be
changed, it is specified in the main config in the
input\_params/emission\_inventories option). In this file, location of
the raw data and metadata is specified together with optional
information on grouping/filtering the raw data.

Any row of the \emph{inventory\_input.txt} file has the following
columns: 
\begin{verbatim}
``inventory\_name'' ``file\_name'' ``file\_path'' ``file\_info\_path''
``set\_name (opt.)'' ``filter (opt.)'' ``filter (opt.)'' ``scenario (opt.)''   
\end{verbatim}


Thus a sample row from TNO inventory looks like this:
\begin{verbatim}
``TNO_III'' ``TNO_MACC_III_v1_1_2011'' ``TNO/TNO_MACC_III_emissions_v1_1_2011_ID.txt'' ``TNO/TNO_MACC_III_emissions_v1_1_2011_ID.info''
``TNO_MACC_III_v1_1_2011_P'' ``SourceType=P''
\end{verbatim}

The file\_info\_path indicates the location of user-provided metadata
\emph{*.info}, where the information on format, source type and geometry
is stored. Below we see a sample info file:

\begin{verbatim}
\# type of file 
file_type = text 
field_delimiter = `,' 
text_delimiter = `"' 
encoding = `utf8'

\# number of lines before header to be skipped 
skip_lines = 0

\# source type 
src_type = A
category_def = 6 
source_id = ZUJ,

\# geometry 
geom_name = `CZ_ZUJ'
\end{verbatim}

Valid values for source type and geometry are stored in the file
\emph{configspec-sources.conf}. Thus, for example, valid values for type
are A(area sources), P(point sources) and L(line sources).

For technical reasons it is handy to include the description of geometry
files at the beginning of the file \emph{inventory\_input.txt}. These
rows have empty name of inventory. In this way sharing of geometry among
different sources is easily treated.

main config file : output files and other paths. The paths must be
created in advance.


\section{The workflow}\label{the-workflow}

In the configuration file fume\_workflow.conf the user may specify the
actual workflow of the simulation. It is possible to comment out some
steps of the simulation which are thus skipped by the processor. This
enables the user to tune single steps without having to go through all
the processes every time. In this case the switch scratch in the main
config file is to be set on False.

The only step which cannot be skipped (if any row after that line is
switched on) is case.prepare\_conf.

\section{Transformations}\label{transformations}

these do not change values, e.g. of emissions. Any transformation may be
restricted to a inventory, set or category.

The transformation configuration file , (e.g. fume\_transformations.conf
(name specified in the main configuration file in the section
transformation, parameter source mozna zmenit na definitions)) specifies
or defines (running later in chains) transformation ``objects''.

valid types of tranformations:

\begin{itemize}
\item
  intersect
\item
  mask (here the masking condition is phrased as a SQL condition??)
\item
  source\_filter : filtering according to source parameters - category
  of source, \ldots{} anything which is in table of sources (doplnit)
\item
  geometrical transformations (change of domain, projection, regridding)
\end{itemize}

Transformations are general but they may be confined to inventory or
set.

A built-in transformation to\_grid performs the intersect with the grid
ep\_grid\_tz (fixed name of the database table, this grid is the target
grid of the user)

The syntax of any section in the transformation configuration file is
derived from the definition file configspec-transformations.conf (the
first row involves the name of the transformation which serves as
reference in the chain specification (see below) in the main config
file):

\begin{verbatim}
 [[name_of_transformation]]
 type = type_of_transformation 
 intersect = name_of_the_shapefile #(must be imported among geometries) 
 mask\_type = inside
 ...
\end{verbatim}

As in other files, the defaults are listed in the definition file
configspec-transformations.conf, too.

\textbf{Running transformations:}

In the main configuration file the so called ``chains'' are defined.
Single chains are intrinsically independent, i.e. they may run in
parallell and no interactions between different chains occurs. In
particular, any chain doesn't overwrite outputs of another chain, but
the output data are all stored in the database. Nevertheless, every
chain should have the to\_grid transformation at the end, otherwise the
resulting emissions are not written into the output file.

in the section {[}transformations{]}, subsection {[}{[}chains{]}{]}

The simplest chain is a mere transformation to user grid. In our case,
this is the {[}{[} to\_grid {]}{]} built-in transformation. This is
written as e.g. chain1 = to\_grid

\section{Time disaggregation and speciation}\label{time-disaggregation-and-speciation}

These follow the processing of sources and all spatial transformations.

\section{Vertical distribution of emissions}\label{sec:vdist}

Along with the emissions inventories often the information how these emissions, 
depending on the activity, are distributed in vertical. FUME implements thus the possibility
to distribute the source emissions to different (user-defined) layers using category-specific
 vertical distribution (vdist) factors. Similar to scenarios, user can define several vertical distributions. These can be applied on chosen emission set (eset). More vertical distributions on each emission set are possible.

The information whether a vdist is to be applied on a eset, is to be set on the optional 8th column in the the emission inventory list file (the 7th columns is the scenario columns, also optional, see \ref{sec::scen}).

In the following example, the vdist "vd1" is applied to the eset R1-area:

\begin{verbatim}
inv_name   file_name  file_path      file_info_path  set_name  filter  scenario	vdist
R1         R1-area    area_emis.csv  area_emis.info  R1-area           sc1	vd1
\end{verbatim}

Additionaly, user needs to provide vdist listing file. This file is a simple list of the defined vertical distributions. Its name and location is defined by user in the main config file by parameter \verb|vertical_distribution_list| in \verb|input_params| section. It contains two columns --- a name of the vertical distribution and a name of the file with its definition. Example of a vdist list file:
\begin{verbatim}
name, file
vd1, vdist1.csv
vd2, vdist2.csv
\end{verbatim}

The columns need to have this order and the header line must be present. The vdist definition files are assumed to be in the same directory as vdist list file. Each vdist definition file contains four columns. Example of a vdist definition file (e.g. vdist1.csv):

\begin{verbatim}
cat_id,level,height,factor
2000000,0,30,0.5
2000000,1,100,0.3
2000000,2,300,0.2
3000000,0,30,0.9
3000000,1,100,0.1
\end{verbatim}

The columns must appear in this order. The first indicates the category for which the vdist factor to be applied, the 2nd columns tell to which output layer with the 3rd column indicating the layer interface height in meters (layer top). Layers are indexed from zero and the 0th layer is adjacent to the surface (30 meters thick in this example). The last columns is the multiplicative factor to be applied on emissions. To avoid loosing emissions, the factors for a particular category must sum up to one.

The switch called \verb|apply_vdistribution| under the  \verb|vdistribution_params| subsection of the \verb|run_params| section is used to control whether to apply vertical distributions. If it is set to "no", vertical distributions are not applied regardless of any definition at an eset line in the emissions inventory list file. The default value is "no". However, FUME behaves consistently here and if it is "yes" and no vdist is defined for any eset, the resulting emissions will be still correctly calculated (and will be surface emissions only).

The vertical distribution heights can be different for different inventories, so FUME implements a the step called \verb|case.process_vertical_distributions| in the FUME workflow file, which recalculates the inventory specific vdist factors to all ancestral emissions categories and to the user defined output layers. If this step in the workflow file is omitted, FUME will not apply any vdist factors. The indicated step in the workflow should be called just after the transformations.

The application of vdist factors take place in the output module (see \ref{output}) when retrieving speciated category specific not yet time-dissaggregated emission totals (e.g. to write as a netcdf file). 


\section{Scenarios}\label{scenario}
FUME enables a possibility to apply emission scenarios. This is implemented as a simple factor and operation, which is applied based on predefined specie(s), category(ies) and attribute filter.  User can define several scenarios. These can be applied on chosen emission set (eset). More scenarios on each emission set are possible. There are two ways of applying the scenario(s): immediately on source import and/or during the transformation chains. In case of scenarios on import, the emission set is stored with the scenario applied already. This is defined in the emission inventory list file (\verb|emission_inventories|) in 7th optional column, where desired scenario names are given (comma separated). Here is an example, where scenario sc1 will be applied on emission set R1-area:
\begin{verbatim}
inv_name   file_name  file_path      file_info_path  set_name  filter  scenario
R1         R1-area    area_emis.csv  area_emis.info  R1-area           sc1
\end{verbatim}

Additionaly, user needs to provide scenario definition files. One file is a simple list of the defined scenarios. Its name and location is defined by user in the main config file by parameter \verb|scenarios_list| in \verb|input_params| section. It contains two columns --- a name of the scenario and a name of the file with its definition. Example of scenario list file:
\begin{verbatim}
name, file
sc1, scenario1.csv
sc2, scenario2.csv
\end{verbatim}
The columns need to have this order and the header line must be present. The scenario definition files are assumed to be in the same directory as scenario list file. Each scenario definition file contains minimum one and maximum five columns. Example of scenario definition file (e.g. scenario1.csv):
\begin{verbatim}
category, species, filter             , value, operation
2020204,  NMVOC,                      , 0.8  , *
       ,  SO2,     REGION_NAME=Region2, 5    , +
\end{verbatim}
The columns can be in arbitrary order, but the header line with exact column names must be present. Whole columns, excepting value, can be ommited, which is the same as empty value. In case of operation column, the default value is multiplication. The above example defines scenario which will multiply emissions of NMVOC for all sources with category 2020204 by 0.8. Additionaly, it will add 5 to emissions of SO2 for all categories, but only for sources in compliance with attribute filter criteria that their REGION\_NAME is Region2. Keep in mind that scenario factors are applied after inner unit conversion. Thus the emission unit is g/s. As attribute filter criteria can be used any column from original raw emission inputs. The category hierarchy is working for scenario values. This means that for each category/specie combination the value is filled for all child categories unless they have its own definition. The simplest scenario definition is as follows
\begin{verbatim}
value
2
\end{verbatim}
and means that all sources of all categories and all species of given eset will be doubled. The factors cannot be ``chained'' in one scenario --- thus defining more factors for one source/specie/category combination is not allowed and this definition
\begin{verbatim}
category, species, filter             , value
2020204,  SO2,     REGION_NAME=Region2, 0.8
       ,  SO2,                        , 5
\end{verbatim}
will result in an error. Although in case those two lines where in two different scenario files and then applied on one eset, both will apply.

The use of scenarios during the transformation chains is possible in two ways:
First, global scenarios applied to all processed inventories can be run by
setting the \verb|all_emissions| option within the \verb|scenarios| subsection of the \verb|run_params| section in the main configuration file
as follows:
\begin{verbatim}
[[scenarios]]
all_emissions=sc1,sc2
\end{verbatim}

In a more complex situation, scenarios can be applied to individual
transformation chains by using the built-in \verb|scenarios| transformation with the
space-separated list of scenario names listed after a colon, e.g.:

\begin{verbatim}
[transformations]
[[chains]]
tno_all=tno_all,scenarios:industry agri
\end{verbatim}

specifies that the scenarios \verb|industry| and \verb|agri| will be applied
at the end of the \verb|tno_all| chain. Scenarios during transformations are somewhat limited --- the attribute filter is ignored (factor value is applied despite this filter) and operation is defined to be multiplication (despite the value in input file).

The current version imports scenario definitions together with the emission sources thus in case of changing/adding the scenario the whole source importing process has to be run. 

Keep in mind that the scenarios on import applies immediately during the emission import, then the calculation pollutants processes. The scenarios during transformations occur before chemical speciations.

\section{External models}\label{external-models}

In many cases, part of the emissions has to be calculated using
different methods than offered by the emissions preprocessor. E.g.
biogenic emissions are routinely calculated using standalone models;
another example could be the calculation of lightning emissions, emissions of windblown dust,
emissions from domestic heating or emission from agriculture - all
having in common some dependence on meteorological conditions. External
models can be written in any programming language and are called with a
python wrapper (interface).

External models themsleves or their interfaces are placed in

client/models e.g. client/models/model1 client/models/model2

They are defined and configured in the {[}{[}models{]}{]} section of the
main config file {[}{[}models{]}{]} models = `model1', `model2' \# the
comma-separated model list model\_configs = `ep\_model1.conf',
`ep\_model2.conf' \# the comma-separated list the each model
configuration (this is the configuration of the interface in general)

There are some general requirements to include models in the emission
preprocessor: 1) the model interface has to hold the name
client/models/model1/ep\_model1.py 2) within ep\_model1.py, a function
named `run' has to be placed which calls the model itself. 3)
optionally, if the particular model requires some preprocessing, the
`preproc' function has to be specified in
client/models/model1/ep\_model1.py as well 4) configuration
specification may be provided as a configspec file, eg.
client/models/model1/configspec-model1.conf. 
See the "dummy" model provided along with the standard FUME code.

Running the models in the workflow including the optional model
preprocessing is done by placing the following steps:
case.preproc\_external\_models \# for calling each model's preproc
function case.run\_external\_models \# to call the models

Models are often dependent on meteorological input. In order they
recieve the right meteorological input fields, the \_required\_met list
has to be specified in its wrapping module client/models/model1/ep\_model1.py

e.g. for MEGAN this is \_required\_met = {[} `soim1', `soit1', `tas',
`ps', `qas', `wndspd10m', `pr24', `par'{]}, where the list points to the
possible internal meteorological variables.

We use IPCC-abbrevations for internal meteorological variable naming 
\itemize
\item tas - temperature at surface {[}K{]} 
\item ta - 3D temperature {[}K{]} 
\item qas - specific humidity at the surface {[}kg/kg{]}
\item qa - 3D specific humidty
\item rsds - surface incident SW radiation {[}W/m2{]}
\item par - photosyntetically active radiation {[}W/m2{]} 
\item pa - 3D pressure {[}Pa{]} 
\item zf - layer interface heights {[}m{]} 
\item uas - U-wind anemometer height (usually 10m) {[}m/s{]} 
\item vas - V-wind anemometer height (usually 10m) {[}m/s{]} 
\item ua - 3D U-wind {[}m/s{]} 
\item va - 3D V-wind {[}m/s{]} 
\item wndspd - 3D wind speed {[}m/s{]}
\item wndspd10m- wind speed at anemometer height (usually 10m) {[}m/s{]} 
\item pr - precipiation flux {[}kg m-2 s-1{]} 
\item pr24 - accumulated precipitation {[}kg m-2{]} 
\item soim - Soil moisture {[}kg/m3{]}
\item soit - Soil temperature {[}K{]}


External models may depend on "activity" data which are handled within FUME standard workflow. These can be any kind of data representing the amount of some activity or intensity of some property wich determines the strength of the emissiosn flux.

Emission generated by external modules may or may not be combined with the internally calculated emission data.

\section{Output}\label{output}

Emission output in various forms is implemented as a postprocessing
module (\texttt{postproc}). Postprocessing routines can perform:

\begin{itemize}
\item
  output for specific air quality model(s)
\item
  graphical output
\end{itemize}

The postprocessing is switched on by including the row
\begin{verbatim}
postproc.run 
\end{verbatim}
into the workflow.conf file.

Postprocessing is configured in the \texttt{postproc} section of the
main configuration file. First a list of postprocessing routines to be
run must be specified in the \texttt{processors} option, eg.:

\begin{verbatim}
[postproc]
processors = postproc.cmaq.CMAQAreaWriter,
postproc.emissplotter.TotalEmissPlotter
\end{verbatim}

Builtin postprocessing routines include: output into input emission
files for air quality models CMAQ and CAMx, generic NetCDF output and a
simple plotter (using \texttt{matplotlib}). Point emissions in CMAQ or
CAMx format are processed by classes:

\begin{itemize}
\item
  cmaq.CMAQPointWriter
\item
  camx.CAMxPointWriter
\end{itemize}

For the output of area emissions two options are implemented for both
CMAQ and CAMx. The original version provided by classes
\texttt{cmaq.CMAQAreaWriter} and \texttt{camx.CAMxAreaWriter} is more
flexible, allows for simpler mixing with other postprocessing classes
(like plotters) at the expense of higher computational cost.

For larger domains and/or higher number of emissions sources, species,
categories etc., an optimized version is provided by classes

\begin{itemize}
\item
  netcdf.AreaTimeDisaggregator
\item
  cmaq.CMAQAreaTimeWriter
\item
  camx.CAMxAreaTimeWriter
\end{itemize}

These classes require an intermediate NetCDF file containing total
emissions to be created prior to their utilization. For that the
\texttt{netcdf.NetCDFTotalAreaWriter} class is provided. In this
two-step approach, the intermediate file can be generated in the same
FUME run as a time series file (by prepending the
\texttt{NetCDFTotalAreaWriter} class to the list of processors) or in
any following FUME run (in this case the \texttt{NetCDFTotalAreaWriter}
class need not be included in the processors list). Each processor has a
dedicated section in the configuration file (\texttt{netcdfareawriter},
\texttt{cmaqareawriter} and \texttt{camxareawriter} respectively).

Example configuration (writes out the intermediate total emissions file
and then the CMAQ and CAMx emission time series file):

\begin{verbatim}
[postproc]
    processors = postproc.netcdf.NetCDFTotalAreaWriter, postproc.cmaq.CMAQAreaTimeWriter, postproc.camx.CAMxAreaTimeWriter

    [[cmaqareawriter]]
        outfile='CMAQ_input_file.nc'
        vgtyp=1
        vgtop=1
        vglvls=1
    [[netcdfareawriter]]
        undef = -9999.
        totalfile='totals_intermediate.nc'
    [[camxareawriter]]
        outfile='CAMx_input_file.dat'
\end{verbatim}

In a following FUME run, if the total emissions don't change, the
NetCDFTotalAreaWriter can be omitted.

Two simple plotter processor are built in:

\begin{itemize}
\item
  emissplotter.EmissPlotter: for each timestep and each species plots a
  figure
\item
  emissplotter.TotalEmissPlotter: for each species plots a figure of
  total (typically per year) emissions
\end{itemize}

\section{Logging}\label{logging}
Proper logging of the run is not implemented at the moment. There is a possibility to turn on/off the debugging notices in the main config file --- parameter \verb|debug|. 

\section{Reporting}\label{reporting}



\newpage
\appendix 

\section{Description of input files}\label{description-of-input-files}

\subsection{Static x configuration data}\label{static-x-configuration-data}

All static x configuration files are comma separated text files, with
quotation marks to mark character strings. The files have one header
row, with mandatory given column names. The order of the column is
arbitrary, additional columns may be present. With the only exception
which are gspro files, the files cannot contain comments nor empty
lines. Processor is case sensitive.

Path to static (configuration) data is set in config parameter
\emph{input\_params}.\emph{static}. Required files:

\textbf{Mandatory}

\begin{quote}
\begin{itemize}
\item
  \emph{inventory\_species.csv} - list of inventory species, known
  internally by FUME
\item
  \emph{emission\_categories.csv} - list of defined emission sources
  categories
\item
  \emph{model\_list.csv} - list of chemical models (model, version)
\item
  \emph{mechanism\_list.csv} - list of chemical mechanisms
\item
  \emph{model\_specie\_names.csv} - list of model species (known for
  model, version, mechanism)
\item
  \emph{sp\_species.csv} - list of speciation species for each chemical
  mechanism
\item
  \emph{tv\_def.csv} - time variation prolie definitions
\item
  \emph{tv\_values.csv} - time profile values
\item
  \emph{tv\_mapping.csv} - category-time variation profile assignment
\end{itemize}
\end{quote}

For \textbf{speciations} either

\begin{quote}
\begin{itemize}
\item
  \emph{gspro files} - files with specie split factors
\end{itemize}
\end{quote}

or

\begin{quote}
\begin{itemize}
\item
  \emph{speciation\_profiles.csv} - compound category profiles, only
  required if speciation profiles (gspro files) are to be created by EP
  from basic compounds
\item
  \emph{compounds.csv} - list of known profile compounds, this is read
  when \emph{speciation\_profiles.csv} is given
\item
  \emph{comp\_mechanisms\_assignment.csv} - assignment of compounds to
  chemical mechanisms, this is read when \emph{speciation\_profiles.csv}
  is given
\end{itemize}
\end{quote}

or both.

\textbf{Optional}

\begin{quote}
\begin{itemize}
\item
  \emph{sp\_mod\_specie\_mapping.csv} - maps speciation species to model
  species
\item
  \emph{tv\_series.csv} - explicit time factors
\end{itemize}
\end{quote}

\subsubsection{General definition inputs}\label{general-definition-inputs}

All files with general input definitions are expected to be in directory
defined in \emph{static} parameter.

\begin{description}
\item[\emph{inventory\_species.csv}]
is a list of input emission species, known internally by FUME, unified
between different emission inventories. The mapping between each
emission inventory specie names is given for each inventory (see
Emissions section). This allows to unify naming conventions across
inventories like PM25, PM2\_5 to be treated as one specie under one
name. The file has two columns: name, description. Name defines the
specie name, description can be empty. All species used in speciations
as inventory specie or in calculate\_pollutants file must be listed
here.
\item[\emph{emission\_categories.csv}]
defines the emission category (tree) hierarchy, known internally by EP.
Similarly to species those categories are unified across different
inventories through mapping file (see Emissions section). The file has
three columns: cat\_id, name, parent. cat\_id is category id, is integer
and each category has defined parent category. This is used for
speciation and time dissagregation. In case the speciation or time
profile is not found for given source category, FUME searches for the
profile of parent category. This is recursive, thus the finally used
profile can be several levels above the given category. If no speciation
or time profile is found for any parent the sources of this category
will not be on output (no warning given). parent\_id MUST be lower
number than cat\_id for correct recursive parent category search.
\item[\emph{model\_list.csv}]
is a list of used chemical models. The file has two columns: model,
version. This is user defined model name/version. Any model name
appearing in othet input files/config must be listed here.
\end{description}

\subsubsection{Speciation inputs}\label{speciation-inputs}

All speciation input files are expected to be in
\emph{static}/speciations directory.

\begin{description}
\item[\emph{model\_specie\_names.csv}]
list of model species (known for model, version, mechanism). The file
has five columns: model, version, mechanism, name, description. This
species will on output for defined model/mechanism.
\item[\emph{sp\_species.csv}]
list of speciation species for each chemical mechanism. The file has
three columns: mechanism\_name, name, carbons (not used). Each
speciation specie used in speciation process must be listed here.
\end{description}

There are two ways of defining speciation profiles that can be combined
together.

\begin{quote}
\begin{enumerate}
\item
  \begin{description}
  \item[gspro files]
  \begin{description}
  \item[\emph{gspro files}]
  files with specie split factors, it has 6 columns

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    column \ldots{} category for which the profile is defined
  \item
    column \ldots{} inventory specie (``input'' for chemical speciation)
  \item
    column \ldots{} chemical mechanism specie (``output'' from chemical
    speciation)
  \item
    column \ldots{} ratio of the output specie in input specie. For
    mole-based speciation (typically used for gases) this is the number
    of moles of output specie in 1 mole of input specie (mole split
    factor). In case of mass-based speciation (typically used for
    aerosol) this is simply mass ratio of output in input.
  \item
    column \ldots{} average molar weight of species in profile for
    gases, 1 for aerosols
  \item
    column is not used in FUME at the moment
  \end{enumerate}

  The output specie is calculated as: mass of input specie * column 4 /
  column 5 . In case of gases resulting value is in moles, in case of
  aerosol it is in grams.
  \end{description}
  \end{description}
\item
  \begin{description}
  \item[profiles of basic compounds (gas profiles) (there is probably a
  bug in treating NVOL/UNR, I do not recommend to use)]
  Those files are consistent with output files of Chemical Speciation
  Database by Carter, 2015 (J Air Waste Manag Assoc, 65 (10)) for CMAS
  Speciation Tool. Those files are only required if speciation profiles
  are to be created by FUME from basic compounds.
  \emph{speciation\_profiles.csv} compound category profiles

  \begin{description}
  \item[\emph{compounds.csv}]
  list of known profile compounds, this is read when
  \emph{speciation\_profiles.csv} is given
  \item[\emph{comp\_mechanisms\_assignment.csv}]
  assignment of compounds to chemical mechanisms, this is read when
  \emph{speciation\_profiles.csv} is given
  \end{description}
  \end{description}
\end{enumerate}
\end{quote}

Where to get chemical speciation files? FUME supports input of
speciation profiles at the same format as SMOKE, i.e. gspro files. In
case user has ready gspro files, they can be used directly for FUME
input. In case the user needs to create his own speciation inputs, the
process is different for aerosols and gases. Aerosols are typically
mass-based splited and thus the gspro input is rather straightforward.
The user needs to obtain information about aerosol composition of
sources of interest somewhere - e.g. TNO supplements its emission
database with basic aerosol composition. Considering the user wants to
define 20 \% OC in PM\_25 for category 100 the corresponding line in
gspro file is 100,PM\_25,OC,0.2,1,\textless{}anything\textgreater{}

For gases, as mole-based speciated species, the situation is more
complicated. The typical situation is that inventory contains VOC (or
equivalent like NMVOC or TOG) which need to be speciated to model
species like ALK, ALD etc. depending on the model chemical mechanism. To
do this user needs to know chemical compound composition for different
categories (e.g. authors employed the information in Passant (2002):
Speciation of UK emissions of non-methane volatile organic compounds).
Then the assignment of compounds to model chemical mechanisms needs to
be given. This assignment is nontrivial. Authors used the Chemical
Speciation Database by Carter, 2015 (J Air Waste Manag Assoc, 65 (10).
This tool can be used in two ways. It can generate gspro files from
given chemical compound profiles, which can be input for FUME. Or,
alternatively, this tool can be used to generate compounds.csv and
comp\_mechanisms\_assignment.csv files as mentioned before and FUME will
use this information to process the chemical speciation itself. GSPRO
files can be also generated by EPA's software Speciation Tool (available
at www.cmascenter.org).

\subsubsection{Temporal variation inputs}\label{temporal-variation-inputs}

Time disaggregation can be given as factors for hours, days in week and
month. This factor is defined as variation from average value for each
time period. The second possibility to define time variation is to give
explicit hourly factors (time serie) for period of interest. Both
possibilities can be combined together.

\subsubsection{Shp time zones}\label{shp-time-zones}

File tz\_world\_mp.shp, included in example data has to be imported in
order to correctly deal with time zones.

\subsection{Emissions}\label{emissions}

Directory where input data for emission sources are stored is defined by
configspec parameter \emph{sources} (default is set same as \emph{path}
parameter).

List of all emission files to be imported are in configspec parameter
\emph{emission\_inventories}.

Description of emission\_inventories file. This file is a txt file, has
exactly one header line and is tab separated. It has 5 columns -
inventory name, inventory version, short name for imported file, file
name to be imported (it is expected to be in sources directory) and
infofile expected in the same place. Lines beginning by \# are ignored.

category + species files

emissions input files can be imported in one of two formats: text file,
shapefile

\subsubsection{inventory inputs list}\label{inventory-inputs-list}

This file is a txt file, has exactly one header line and is tab
separated. It has 5 columns - inventory name, inventory version, short
name for imported file, file name to be imported (it is expected to be
in input.sources directory) and infofile expected in the same place.
Similarly to configuration text files any blank or comment lines are ignored.

\subsubsection{Emission text files}

Similarly to other input text files it can have arbitrary number of introductory blank or comment lines before one header line. On contrary, no blank or comment lines should come after the header line.

\begin{quote}
\textbf{met} TODO
\end{quote}

MEGAN TODO

\end{document}

